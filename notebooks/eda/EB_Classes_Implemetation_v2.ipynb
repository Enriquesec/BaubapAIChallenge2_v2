{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66594192-f00c-481d-8bf0-463935754427",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66594192-f00c-481d-8bf0-463935754427",
    "outputId": "158d34fa-e749-41a3-cb2e-8da31e4ae881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from category_encoders) (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from category_encoders) (1.4.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from category_encoders) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from category_encoders) (1.24.4)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from category_encoders) (1.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2022.1)\n",
      "Requirement already satisfied: six in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mi16690\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mi16690\\anaconda3\\lib\\site-packages (from packaging>=21.3->statsmodels>=0.9.0->category_encoders) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats as st\n",
    "import category_encoders as ce\n",
    "#----------------------CategoricalTransformer----------------------\n",
    "\n",
    "class CategoricalTransformer():\n",
    "    def __init__(self, max_unique_values = 20):\n",
    "        self.max_unique_values = max_unique_values\n",
    "        self.__selected_features = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Identify features with 20 or fewer unique values\n",
    "        self.__selected_features = X.columns[X.nunique() <= self.max_unique_values]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        # Convert selected features to categorical\n",
    "        X_transformed[self.__selected_features] = X_transformed[self.__selected_features].astype('object')\n",
    "        return X_transformed\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------MissingValuesFiller----------------------\n",
    "\n",
    "class MissingValuesFiller():\n",
    "    def __init__(self, numeric_strategy='mean', categorical_strategy='most_frequent'):\n",
    "        self.numeric_strategy = numeric_strategy\n",
    "        self.categorical_strategy = categorical_strategy\n",
    "        self.__numeric_cols = None\n",
    "        self.__categorical_cols = None\n",
    "        self.__mean_numeric_series = None\n",
    "        self.__mode_categorical_series = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Separate numeric and categorical columns\n",
    "        self.__numeric_cols = X.select_dtypes(include='number').columns\n",
    "        self.__categorical_cols = X.select_dtypes(include='object').columns\n",
    "        self.__mean_numeric_series= X[self.__numeric_cols].agg(self.numeric_strategy)\n",
    "        self.__mode_categorical_series= X[self.__categorical_cols].mode().iloc[0] if self.categorical_strategy  == 'most_frequent' else self.categorical_strategy\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "         # Fill missing values in numeric columns\n",
    "        X_transformed[self.__numeric_cols] = X[self.__numeric_cols].fillna(self.__mean_numeric_series)\n",
    "\n",
    "        # Fill missing values in categorical columns\n",
    "        X_transformed[self.__categorical_cols] = X[self.__categorical_cols].fillna(self.__mode_categorical_series)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------DropHighCorrelationFeatures----------------------\n",
    "\n",
    "\n",
    "class DropHighCorrelationFeatures():\n",
    "    def __init__(self, threshold = 0.85):\n",
    "        self.threshold = threshold\n",
    "        self.to_drop_cols = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Calculate the correlation matrix\n",
    "        correlation_matrix = X.corr().abs()\n",
    "\n",
    "        # Create a mask to identify highly correlated features\n",
    "        upper_triangle_mask = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify features with correlation above the threshold\n",
    "        to_drop = [column for column in upper_triangle_mask.columns if any(upper_triangle_mask[column] > self.threshold)]\n",
    "\n",
    "        self.to_drop_cols = to_drop\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Drop highly correlated features\n",
    "        X_transformed = X_transformed.drop(columns=self.to_drop_cols)\n",
    "\n",
    "        return X_transformed\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "#----------------------SplitterByType----------------------\n",
    "\n",
    "class SplitterByType():\n",
    "    def __init__(self):\n",
    "        self.__numeric_cols = None\n",
    "        self.__categorical_cols = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Identify numeric and categorical columns\n",
    "        self.__numeric_cols = X.select_dtypes(include='number').columns\n",
    "        self.__categorical_cols = X.select_dtypes(include='object').columns\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Create DataFrames based on column types\n",
    "        numeric_df = X[self.__numeric_cols].copy()\n",
    "        categorical_df = X[self.__categorical_cols].copy()\n",
    "\n",
    "        return numeric_df, categorical_df\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "#----------------------ConstantsAndHighCardinalityDropper----------------------\n",
    "class ConstantsAndHighCardinalityDropper():\n",
    "    def __init__(self,max_unique_values=2000):\n",
    "        self.max_unique_values = max_unique_values\n",
    "        self.__non_constant_cols = None\n",
    "        self.__high_cardinality_cols = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Identify numeric and categorical columns\n",
    "        self.__non_constant_cols = X.columns[X.nunique() > 1]\n",
    "        self.__high_cardinality_cols = X.columns[X.nunique() > self.max_unique_values]\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Get non constants columns\n",
    "        X_transformed = X_transformed[self.__non_constant_cols]\n",
    "\n",
    "        #drop high cardinality columns\n",
    "        X_transformed = X_transformed.drop(columns=self.__high_cardinality_cols)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "#----------------------ConstantsAndHighCardinalityDropper----------------------\n",
    "class FrequencyEncoder():\n",
    "    def __init__(self):\n",
    "        self.categorical_features = None\n",
    "        #dictionary with values of the categorical features as keys and dictionary of frequencies\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Identify numeric and categorical columns\n",
    "        self.categorical_features = X.select_dtypes('object').columns\n",
    "\n",
    "        for cat_col in self.categorical_features:\n",
    "            class_values_frequency = {}\n",
    "            for class_value in X[cat_col].unique():\n",
    "                class_values_frequency[class_value] = sum(X[cat_col]==class_value)/X.shape[0]\n",
    "            self.mapper[cat_col] = class_values_frequency\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # apply frequencies mapping\n",
    "        for cat_col in self.categorical_features:\n",
    "            X_transformed[cat_col] = X_transformed[cat_col].map(self.mapper[cat_col])\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c0c39b7-f818-443a-aa30-3548c776290d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "4c0c39b7-f818-443a-aa30-3548c776290d",
    "outputId": "2ee290a9-d7bc-41cc-cd24-0e358b06b0ac"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/training.csv')\n",
    "X = df.iloc[:,:-1].copy()\n",
    "y = df.Target\n",
    "X_val1 = pd.read_csv('../data/raw/cap_data_validation1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e7339-a9ca-46de-aac0-eac6b3151141",
   "metadata": {
    "id": "e33e7339-a9ca-46de-aac0-eac6b3151141"
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a22b12db-0aa1-4dfe-a574-84443ca7f8fb",
   "metadata": {
    "id": "a22b12db-0aa1-4dfe-a574-84443ca7f8fb"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X: pd.DataFrame, X_val: pd.DataFrame, numeric_strategy='mean',categorical_strategy='most_frequent',\n",
    "                   threshold=0.85,cat_transformer_max_unique_values = 20,cardinality_max_unique_values=2000,\n",
    "                   n_components = 40, cat_encoder_strategy = 'woee'):\n",
    "    # Step 0: Copy Values to transform\n",
    "    X_transformed = X.copy()\n",
    "    X_val_transformed = X_val.copy()\n",
    "\n",
    "    # Step 1: Fill missing values\n",
    "    filler = MissingValuesFiller(numeric_strategy=numeric_strategy, categorical_strategy=categorical_strategy)\n",
    "    X_transformed = filler.fit_transform(X_transformed)\n",
    "    X_val_transformed = filler.transform(X_val_transformed)\n",
    "\n",
    "    # Step 2: Reduce dimensionality due to high correlation\n",
    "    correlation_reducer = DropHighCorrelationFeatures(threshold=threshold)\n",
    "    X_transformed = correlation_reducer.fit_transform(X_transformed)\n",
    "    X_val_transformed = correlation_reducer.transform(X_val_transformed)\n",
    "\n",
    "    # Step 3: Convert features to categorical\n",
    "    categorical_transformer = CategoricalTransformer(max_unique_values = cat_transformer_max_unique_values).fit(X_transformed)\n",
    "    X_transformed = categorical_transformer.transform(X_transformed)\n",
    "    X_val_transformed = categorical_transformer.transform(X_val_transformed)\n",
    "\n",
    "\n",
    "    # Step 4: Drop constants and high-cardinality features\n",
    "    dropper = ConstantsAndHighCardinalityDropper(max_unique_values = cardinality_max_unique_values)\n",
    "    X_transformed = dropper.fit_transform(X_transformed)\n",
    "    X_val_transformed = dropper.transform(X_val_transformed)\n",
    "\n",
    "\n",
    "    # Step 5: Split the DataFrame into numeric and categorical parts\n",
    "    splitter = SplitterByType()\n",
    "    X_transformed_numeric, X_transformed_categorical = splitter.fit_transform(X_transformed)\n",
    "    X_val_transformed_numeric, X_val_transformed_categorical = splitter.transform(X_val_transformed)\n",
    "    incosistencias_columns = ['Feature_11', 'Feature_71', 'Feature_88', 'Feature_90',\n",
    "       'Feature_143', 'Feature_156', 'Feature_176', 'Feature_194',\n",
    "       'Feature_200', 'Feature_216', 'Feature_237', 'Feature_268',\n",
    "       'Feature_273', 'Feature_319', 'Feature_339', 'Feature_386',\n",
    "       'Feature_391', 'Feature_402', 'Feature_407', 'Feature_410',\n",
    "       'Feature_417', 'Feature_421', 'Feature_422', 'Feature_427',\n",
    "       'Feature_431', 'Feature_441', 'Feature_471', 'Feature_477',\n",
    "       'Feature_492', 'Feature_532']\n",
    "    col_new = set(X_transformed_numeric)-set(incosistencias_columns)\n",
    "    X_transformed_numeric = X_transformed_numeric[col_new]\n",
    "    X_val_transformed_numeric = X_val_transformed_numeric[col_new]\n",
    "\n",
    "    # Step 6: Reduce dimensionality of numeric features\n",
    "    pca = PCA(n_components)\n",
    "    X_transformed_numeric = pd.DataFrame(pca.fit_transform(X_transformed_numeric))\n",
    "    X_val_transformed_numeric = pd.DataFrame(pca.transform(X_val_transformed_numeric))\n",
    "    print(f'Explained Variance with {n_components} components: {pca.explained_variance_ratio_.sum()}')\n",
    "\n",
    "\n",
    "    # Step 7: Categorical Encoding\n",
    "    if cat_encoder_strategy == 'woee':\n",
    "        # Step 7.1 (Optional): Apply WOE Encoding\n",
    "        X_columns = X_transformed_categorical.columns\n",
    "        woe_encoder = ce.WOEEncoder(cols=X_columns)\n",
    "\n",
    "        X_transformed_categorical = woe_encoder.fit_transform(X_transformed_categorical, y)\n",
    "        X_val_transformed_categorical = woe_encoder.transform(X_val_transformed_categorical)\n",
    "    elif cat_encoder_strategy == 'freq':\n",
    "        # Step 7.2 (Optional): Apply self defined Frequency Encoding\n",
    "        X_columns = X_transformed_categorical.columns\n",
    "        freq_encoder = FrequencyEncoder()\n",
    "\n",
    "        X_transformed_categorical = freq_encoder.fit_transform(X_transformed_categorical)\n",
    "        X_val_transformed_categorical = freq_encoder.transform(X_val_transformed_categorical)\n",
    "    else:\n",
    "        # Step 7.3 (Optional): Apply One Hot Encoding\n",
    "        X_columns = X_transformed_categorical.columns\n",
    "        ohe_encoder = ce.OneHotEncoder()\n",
    "\n",
    "        X_transformed_categorical = ohe_encoder.fit_transform(X_transformed_categorical)\n",
    "        X_val_transformed_categorical = ohe_encoder.transform(X_val_transformed_categorical)\n",
    "\n",
    "    # Step 8: Mixing steps 6 & 7 to prepare data for modelling\n",
    "    X_transformed = pd.concat([X_transformed_numeric,X_transformed_categorical], axis=1)\n",
    "    X_val_transformed = pd.concat([X_val_transformed_numeric,X_val_transformed_categorical], axis=1)\n",
    "\n",
    "    return X_transformed, X_val_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7b1e5-680b-4784-b506-9078d4feb9a7",
   "metadata": {
    "id": "c3f7b1e5-680b-4784-b506-9078d4feb9a7"
   },
   "source": [
    "**Next steps... concatenate dataframes applying encoders to categorical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e756e87-282b-45d6-b8b8-964d5b7be714",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "5e756e87-282b-45d6-b8b8-964d5b7be714",
    "outputId": "2408f8e1-a404-42f5-aef9-182c0d6b9ca6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MI16690\\AppData\\Local\\Temp\\1\\ipykernel_22548\\984988466.py:43: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X_transformed_numeric = X_transformed_numeric[col_new]\n",
      "C:\\Users\\MI16690\\AppData\\Local\\Temp\\1\\ipykernel_22548\\984988466.py:44: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X_val_transformed_numeric = X_val_transformed_numeric[col_new]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance with 40 components: 0.8276145411905154\n"
     ]
    }
   ],
   "source": [
    "X_transformed, X_val_transformed = preprocess_data(X, X_val1, cat_encoder_strategy='ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a44c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88ffe9f5-4aa6-4ae2-a7b7-16f392b09e97",
   "metadata": {
    "id": "88ffe9f5-4aa6-4ae2-a7b7-16f392b09e97"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebb11c56-0b1b-4a00-a223-e5388d7f2060",
   "metadata": {
    "id": "ebb11c56-0b1b-4a00-a223-e5388d7f2060"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, brier_score_loss, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "# Create a function that return the metrics we need to compare if the model is a good one.\n",
    "# The function returns 'brier_score', 'confusion matrix', 'accuracy negative and positive', 'accuracy'\n",
    "def metricas_modelo(y_real, prob_pred):\n",
    "    y_pred = list(map(lambda x: 1 if x==True else 0, prob_pred>0.5))\n",
    "    brier = round(brier_score_loss(y_real,prob_pred)*100,2)\n",
    "    cm = confusion_matrix(y_real,y_pred)\n",
    "    acc_neg = round(100*cm[0,0]/(cm[0,0]+cm[0,1]),2)\n",
    "    acc_pos = round(100*cm[1,1]/(cm[1,0]+cm[1,1]),2)\n",
    "    acc = round(100*accuracy_score(y_real, y_pred),2)\n",
    "    print(f' brier: {brier},\\n confusion_m: \\n{cm},\\n acc_neg: {acc_neg},\\n acc_pos: {acc_pos},\\n accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ca3e8-56dd-4031-80e2-fdbc8cfd19b4",
   "metadata": {
    "id": "885ca3e8-56dd-4031-80e2-fdbc8cfd19b4"
   },
   "source": [
    "Usando metodología WOEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f659a9b0-79fe-450c-8e35-5d9ebd5b00c5",
   "metadata": {
    "id": "f659a9b0-79fe-450c-8e35-5d9ebd5b00c5",
    "outputId": "a6b32513-a0fc-498d-9691-02ba5f313ce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " brier: 10.16,\n",
      " confusion_m: \n",
      "[[2050   12]\n",
      " [ 240    5]],\n",
      " acc_neg: 99.42,\n",
      " acc_pos: 2.04,\n",
      " accuracy: 89.08\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.90, random_state=42)\n",
    "\n",
    "# Crear un objeto DMatrix para los datos de entrenamiento y prueba\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definir los parámetros del modelo\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# Entrenar el modelo\n",
    "num_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "\n",
    "metricas_modelo(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb4cef-f2c3-4a6d-9605-6d94e8f12795",
   "metadata": {
    "id": "90fb4cef-f2c3-4a6d-9605-6d94e8f12795"
   },
   "source": [
    "Usando metodología  basada en frecuencias"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
